### åˆ˜äºŒå¤§äºº

<script>
Â Â MathJaxÂ =Â {
Â Â Â Â tex:Â {
Â Â Â Â Â Â inlineMath:Â [['$',Â '$'],Â ['\\(',Â '\\)']],
Â Â Â Â Â Â displayMath:Â [["$$",Â "$$"],Â ["\\[",Â "\\]"]],
Â Â Â Â },
Â Â Â Â svg:Â {
Â Â Â Â Â Â fontCache:Â 'global'
Â Â Â Â }
Â Â };
</script>
<scriptÂ type="text/javascript"Â id="MathJax-script"Â async
Â Â src="https://static-621585.oss-cn-beijing.aliyuncs.com/mathjax-v3.js">
</script>

#### å‚è€ƒèµ„æ–™
* [åˆ˜äºŒå¤§äºº-cnblog](https://www.cnblogs.com/zhouyeqin/category/2231506.html)
* [åˆ˜äºŒå¤§äºº-zhihu](https://zhuanlan.zhihu.com/p/166104074)
* [numpyåœ¨çº¿è¿è¡Œ](https://onecompiler.com/python/3zqwrqg2r)

#### ç¬¬äºŒè®²-çº¿æ€§æ¨¡å‹
* MSEï¼šå‡å€¼å¹³æ–¹è¯¯å·®(Mean Square Error)ï¼Œ$MSE=\frac{1}{N} \sum\limits_{n=1}^N (\hat{y}_n - y_n)^2$
* è®­ç»ƒé›†(1,2)ã€(2,4)ã€(3,6)ï¼Œç”¨ç©·ä¸¾æ³•é¢„æµ‹x=4æ—¶ï¼Œyçš„å€¼ï¼š
  * å‡è®¾æ¨¡å‹æ˜¯y = ğœ” * xï¼Œæ±‚ğœ”çš„å€¼
  * for (ğœ”=0; ğœ”<4.1; ğœ”+=0.1)ï¼Œä¾æ¬¡æ±‚å‡ºMSEï¼Œå–MSEæœ€ä½æ—¶ğœ”çš„å€¼

#### ç¬¬ä¸‰è®²-æ¢¯åº¦ä¸‹é™æ³•
* è®­ç»ƒé›†(1,2)ã€(2,4)ã€(3,6)ï¼Œç”¨æ¢¯åº¦ä¸‹é™æ³•é¢„æµ‹x=4æ—¶ï¼Œyçš„å€¼ï¼š
* æ¢¯åº¦ä¸‹é™ï¼š
  * å‡è®¾æ¨¡å‹æ˜¯y = ğœ” * xï¼Œæ±‚ğœ”çš„å€¼
  * å…ˆé€‰å®šğœ”=1
  * ä¸‹ä¸€ä¸ª$ğœ”=ğœ”-Î±\frac{ğœ•cost}{ğœ•ğœ”}$ï¼Œå…¶ä¸­Î±è¡¨ç¤ºæ­¥é•¿ï¼Œcostå³MSE
  * ä¸‹ä¸€ä¸ª$ğœ”=ğœ”-Î±\frac{1}{N}\sum\limits_{n=1}^N 2.x_n.(x_n.ğœ” - y_n)$
  * æ­¥é•¿å®šä¸º0.01ï¼Œè¿­ä»£100æ¬¡ï¼Œè§‚å¯ŸMSEå€¼æ˜¯å¦ä¼šæ”¶æ•›
* éšæœºæ¢¯åº¦ä¸‹é™ï¼š
  * å‡è®¾æ¨¡å‹æ˜¯y = ğœ” * xï¼Œæ±‚ğœ”çš„å€¼
  * å…ˆé€‰å®šğœ”=1
  * ä¸‹ä¸€ä¸ª$ğœ”=ğœ”-Î±\frac{ğœ•loss}{ğœ•ğœ”}$ï¼Œå…¶ä¸­Î±è¡¨ç¤ºæ­¥é•¿ï¼Œ$loss = (\hat{y}_n - y_n)^2 \quad \frac{ğœ•loss}{ğœ•ğœ”} = 2.x_n.(x_n.ğœ” - y_n)$
  * æ­¥é•¿å®šä¸º0.01ï¼Œè¿­ä»£100æ¬¡ï¼Œè§‚å¯Ÿlosså€¼æ˜¯å¦ä¼šæ”¶æ•›
* æ¢¯åº¦ä¸‹é™å’Œéšæœºæ¢¯åº¦ä¸‹é™
  * éšæœºæ¢¯åº¦ä¸‹é™éœ€è¦ç­‰å¾…ä¸Šä¸€ä¸ªå€¼è¿è¡Œå®Œæ‰èƒ½æ›´æ–°ä¸‹ä¸€ä¸ªå€¼ï¼Œæ— æ³•å¹¶è¡Œè®¡ç®—
  * éšæœºæ¢¯åº¦ä¸‹é™å¯ä»¥æœ‰æ•ˆè§£å†³éç‚¹é—®é¢˜
  * æŠ˜ä¸­çš„åŠæ³•æ˜¯mini_batch

#### ç¬¬å››è®²-åå‘ä¼ æ’­
* å‡è®¾æ¨¡å‹æ˜¯$y = ğœ” * x$ï¼Œæ±‚ğœ”çš„å€¼ï¼Œ[è§å›¾](../images/back-propagation.png)
* å‡è®¾æ¨¡å‹æ˜¯$y = ğœ” * x + b$ï¼Œ[äººå·¥æ™ºèƒ½åŸç†-æ›²é¢æ¢¯åº¦ä¸‹é™å’Œåå‘ä¼ æ’­](https://blog.csdn.net/wanlin_yang/article/details/129263378)
  * $loss = (ğœ”x + b - y)^2 = x^2ğœ”^2 + (2x.b - 2x.y)ğœ” + (y^2 + b^2 - 2y.b)$
  * $\frac{ğœ•loss}{ğœ•ğœ”} = 2x(ğœ”x + b - y)$
  * $\frac{ğœ•loss}{ğœ•b} = 2(ğœ”x + b - y)$
  * æ¢¯åº¦ä¸‹é™è§£æ³•ï¼šliuer/lesson4_2.py
  * éšæœºæ¢¯åº¦ä¸‹é™ï¼šliuer/lesson4_3.py
* å‡è®¾æ¨¡å‹æ˜¯$y = ğœ” * x + b$ï¼Œè®¡ç®—æŸå¤±å‡½æ•°ç›¸å¯¹äºå„ä¸ªå‚æ•°çš„åå¯¼æ•°æ¥æ±‚è§£æ¢¯åº¦ã€‚
  * $loss = (\hat{y} - y)^2 \quad \hat{y} = ğœ” * x + b$
  * $\frac{ğœ•loss}{ğœ•ğœ”} = \frac{ğœ•loss}{ğœ•\hat{y}}.\frac{ğœ•\hat{y}}{ğœ•ğœ”} = 2(\hat{y} - y).x$
  * $\frac{ğœ•loss}{ğœ•b} = \frac{ğœ•loss}{ğœ•\hat{y}}.\frac{ğœ•\hat{y}}{ğœ•b} = 2(\hat{y} - y).1$
* å‡è®¾æ¨¡å‹æ˜¯$y = ğœ”_1.x^2 + ğœ”_2.x + b$
  * $loss = (\hat{y} - y)^2 \quad \hat{y} = ğœ”_1.x^2 + ğœ”_2.x + b$
  * $\frac{ğœ•loss}{ğœ•ğœ”_1} = \frac{ğœ•loss}{ğœ•\hat{y}}.\frac{ğœ•\hat{y}}{ğœ•ğœ”_1} = 2(\hat{y} - y).x^2$
  * $\frac{ğœ•loss}{ğœ•ğœ”_2} = \frac{ğœ•loss}{ğœ•\hat{y}}.\frac{ğœ•\hat{y}}{ğœ•ğœ”_2} = 2(\hat{y} - y).x$
  * $\frac{ğœ•loss}{ğœ•b} = \frac{ğœ•loss}{ğœ•\hat{y}}.\frac{ğœ•\hat{y}}{ğœ•b} = 2(\hat{y} - y).1$

#### ç¬¬äº”è®²-PyTorchçº¿æ€§å›å½’
* PyTorchçš„å››ä¸ªæ­¥éª¤ï¼šå‡†å¤‡æ•°æ®ã€å®šä¹‰æ¨¡å‹ã€å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ã€è®­ç»ƒå‘¨æœŸ
* è®­ç»ƒå‘¨æœŸçš„ä¸‰ä¸ªæ­¥éª¤ï¼š
  * å‰é¦ˆforwardï¼šè®¡ç®—$\hat{y} \quad loss$
  * åé¦ˆbackwardï¼šåå‘ä¼ æ’­ã€è®¡ç®—æ¢¯åº¦
  * æ›´æ–°updateï¼šæ›´æ–°å‚æ•°

#### ç¬¬å…­è®²-é€»è¾‘æ–¯è°›å›å½’
* [ä¸€ç¯‡æ–‡ç« ææ‡‚logit, logisticå’Œsigmoidçš„åŒºåˆ«](https://zhuanlan.zhihu.com/p/358223959)
* sigmoidå‡½æ•°æ˜¯æŒ‡æŸä¸€ç±»å½¢å¦‚"S"çš„å‡½æ•°ï¼Œä¾‹å¦‚[è¿™äº›å‡½æ•°](../images/sigmoid-function.jpg)
* logisticå‡½æ•°ä¹Ÿæ˜¯sigmoidå‡½æ•°ï¼Œåœ¨PyTorchä¸­sigmoidå‡½æ•°å³æ˜¯logisticå‡½æ•°
* logisticå›å½’è™½ç„¶åä¸ºå›å½’ï¼Œä½†å®é™…ç”¨äºåˆ†ç±»é—®é¢˜ã€‚
* 



