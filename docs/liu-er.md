### åˆ˜äºŒå¤§äºº

<script>
Â Â MathJaxÂ =Â {
Â Â Â Â tex:Â {
Â Â Â Â Â Â inlineMath:Â [['$',Â '$'],Â ['\\(',Â '\\)']],
Â Â Â Â Â Â displayMath:Â [["$$",Â "$$"],Â ["\\[",Â "\\]"]],
Â Â Â Â },
Â Â Â Â svg:Â {
Â Â Â Â Â Â fontCache:Â 'global'
Â Â Â Â }
Â Â };
</script>
<scriptÂ type="text/javascript"Â id="MathJax-script"Â async
Â Â src="https://static-621585.oss-cn-beijing.aliyuncs.com/mathjax-v3.js">
</script>

#### å‚è€ƒèµ„æ–™
* [åˆ˜äºŒå¤§äºº-cnblog](https://www.cnblogs.com/zhouyeqin/category/2231506.html)
* [åˆ˜äºŒå¤§äºº-zhihu](https://zhuanlan.zhihu.com/p/166104074)

#### ç¬¬äºŒè®²-çº¿æ€§æ¨¡å‹
* MSEï¼šå‡å€¼å¹³æ–¹è¯¯å·®(Mean Square Error)ï¼Œ$MSE=\frac{1}{N} \sum\limits_{n=1}^N (\hat{y}_n - y_n)^2$
* è®­ç»ƒé›†(1,2)ã€(2,4)ã€(3,6)ï¼Œç”¨ç©·ä¸¾æ³•é¢„æµ‹x=4æ—¶ï¼Œyçš„å€¼ï¼š
  * å‡è®¾æ¨¡å‹æ˜¯y = ğœ” * xï¼Œæ±‚ğœ”çš„å€¼
  * for (ğœ”=0; ğœ”<4.1; ğœ”+=0.1)ï¼Œä¾æ¬¡æ±‚å‡ºMSEï¼Œå–MSEæœ€ä½æ—¶ğœ”çš„å€¼

#### ç¬¬ä¸‰è®²-æ¢¯åº¦ä¸‹é™æ³•
* è®­ç»ƒé›†(1,2)ã€(2,4)ã€(3,6)ï¼Œç”¨æ¢¯åº¦ä¸‹é™æ³•é¢„æµ‹x=4æ—¶ï¼Œyçš„å€¼ï¼š
* æ¢¯åº¦ä¸‹é™ï¼š
  * å‡è®¾æ¨¡å‹æ˜¯y = ğœ” * xï¼Œæ±‚ğœ”çš„å€¼
  * å…ˆé€‰å®šğœ”=1
  * ä¸‹ä¸€ä¸ª$ğœ”=ğœ”-Î±\frac{ğœ•cost}{ğœ•ğœ”}$ï¼Œå…¶ä¸­Î±è¡¨ç¤ºæ­¥é•¿ï¼Œcostå³MSE
  * ä¸‹ä¸€ä¸ª$ğœ”=ğœ”-Î±\frac{1}{N}\sum\limits_{n=1}^N 2.x_n.(x_n.ğœ” - y_n)$
  * æ­¥é•¿å®šä¸º0.01ï¼Œè¿­ä»£100æ¬¡ï¼Œè§‚å¯ŸMSEå€¼æ˜¯å¦ä¼šæ”¶æ•›
* éšæœºæ¢¯åº¦ä¸‹é™ï¼š
  * å‡è®¾æ¨¡å‹æ˜¯y = ğœ” * xï¼Œæ±‚ğœ”çš„å€¼
  * å…ˆé€‰å®šğœ”=1
  * ä¸‹ä¸€ä¸ª$ğœ”=ğœ”-Î±\frac{ğœ•loss}{ğœ•ğœ”}$ï¼Œå…¶ä¸­Î±è¡¨ç¤ºæ­¥é•¿ï¼Œ$loss = (\hat{y}_n - y_n)^2 \quad \frac{ğœ•loss}{ğœ•ğœ”} = 2.x_n.(x_n.ğœ” - y_n)$
  * æ­¥é•¿å®šä¸º0.01ï¼Œè¿­ä»£100æ¬¡ï¼Œè§‚å¯Ÿlosså€¼æ˜¯å¦ä¼šæ”¶æ•›
* æ¢¯åº¦ä¸‹é™å’Œéšæœºæ¢¯åº¦ä¸‹é™
  * éšæœºæ¢¯åº¦ä¸‹é™éœ€è¦ç­‰å¾…ä¸Šä¸€ä¸ªå€¼è¿è¡Œå®Œæ‰èƒ½æ›´æ–°ä¸‹ä¸€ä¸ªå€¼ï¼Œæ— æ³•å¹¶è¡Œè®¡ç®—
  * éšæœºæ¢¯åº¦ä¸‹é™å¯ä»¥æœ‰æ•ˆè§£å†³éç‚¹é—®é¢˜
  * æŠ˜ä¸­çš„åŠæ³•æ˜¯mini_batch

#### ç¬¬å››è®²-åå‘ä¼ æ’­
* å‡è®¾æ¨¡å‹æ˜¯y = ğœ” * xï¼Œæ±‚ğœ”çš„å€¼ï¼Œ[è§å›¾](../images/back_propagation.png)
* å‡è®¾æ¨¡å‹æ˜¯y = ğœ” * x + bï¼Œ[äººå·¥æ™ºèƒ½åŸç†-æ›²é¢æ¢¯åº¦ä¸‹é™å’Œåå‘ä¼ æ’­](https://blog.csdn.net/wanlin_yang/article/details/129263378)
  * $loss = (y_n - (ğœ”x_n + b))^2 = x_n^2ğœ”^2 + (2x_n.b - 2x_n.y_n)ğœ” + (y_n^2 + b^2 - 2y_n.b)$
  * $\frac{ğœ•loss}{ğœ•ğœ”} = 2x_n^2ğœ” + (2x_n.b - 2x_n.y_n)$
  * $\frac{ğœ•loss}{ğœ•b} = 2b + (2x_n.ğœ” - 2y_n)$
  * æ¢¯åº¦ä¸‹é™è§£æ³•ï¼šliuer/lesson4_2.py
  * éšæœºæ¢¯åº¦ä¸‹é™ï¼šliuer/lesson4_3.py
* å‡è®¾æ¨¡å‹æ˜¯y = ğœ” * x + bï¼Œè®¡ç®—æŸå¤±å‡½æ•°ç›¸å¯¹äºå„ä¸ªå‚æ•°çš„åå¯¼æ•°æ¥æ±‚è§£æ¢¯åº¦ã€‚
  * $loss = (\hat{y} - y)^2 \quad \hat{y} = ğœ” * x + b$
  * $\frac{ğœ•loss}{ğœ•ğœ”} = \frac{ğœ•loss}{ğœ•\hat{y}}.$\frac{ğœ•\hat{y}}{ğœ•ğœ”} = 2(\hat{y} - y).x$
  * $\frac{ğœ•loss}{ğœ•b} = \frac{ğœ•loss}{ğœ•\hat{y}}.$\frac{ğœ•\hat{y}}{ğœ•b} = 2(\hat{y} - y).1$
* å‡è®¾æ¨¡å‹æ˜¯$y = ğœ”_1.x^2 * ğœ”_2.x + b$
  * $loss = (\hat{y} - y)^2 \quad \hat{y} = ğœ”_1.x^2 * ğœ”_2.x + b$
  * $\frac{ğœ•loss}{ğœ•ğœ”_1} = \frac{ğœ•loss}{ğœ•\hat{y}}.$\frac{ğœ•\hat{y}}{ğœ•ğœ”_1} = 2(\hat{y} - y).x^2$
  * $\frac{ğœ•loss}{ğœ•ğœ”_2} = \frac{ğœ•loss}{ğœ•\hat{y}}.$\frac{ğœ•\hat{y}}{ğœ•ğœ”_2} = 2(\hat{y} - y).x$
  * $\frac{ğœ•loss}{ğœ•b} = \frac{ğœ•loss}{ğœ•\hat{y}}.$\frac{ğœ•\hat{y}}{ğœ•b} = 2(\hat{y} - y).1$

#### ç¬¬äº”è®²-PyTorchçº¿æ€§å›å½’






