### Designing Data-Intensive Application

#### 参考资料
* [我读 DDIA](https://mp.weixin.qq.com/s/prB6R3Awu252D3czbnZUkQ)

#### 第1章 可靠、可扩展与可维护的应用系统
* 设计数据系统会碰到的问题：
  * 如何确保数据的正确性和完整性？
  * 系统降级时如何提供一致的良好表现？
  * 负载增加时系统如何扩展？
  * 友好的服务API如何设计？
* 可靠性：安全的正确的执行，即使发生了某些错误也能正确运行
  * 硬件故障：硬盘崩溃、内存故障、电网停电、网线断了等等
  * 软件错误：特定输入导致的错误；CPU内存资源用尽；依赖的服务出现异常；
  * 人为失误：以最小出错的方式设计系统
* 可扩展性：垂直扩展和水平扩展
* 可维护性：三个设计原则
  * 可运维性，良好的可操作性使工作变得简单，从而关注于高附加值的任务
    * 提供系统运行时的可观测性
    * 支持自动化，与标准工具集成
    * 避免绑定特定机器，允许机器停机维护
    * 良好的文档和易于理解的操作模式
  * 简单性
    * 复杂性的表现：状态空间的膨胀；模块紧耦合；不一致的命名和术语；引入特殊框架
    * 复杂性使得维护系统变得越来越困难，预算超支和进度滞后
    * 好的设计抽象可以隐藏大量的设计细节，对外提供干净易懂的接口
  * 可演化性：目标是可以轻松的修改数据系统，使其适应不断变化的需求
  
#### 第2章 数据模型与查询语言
* 常见的三种数据模型：关系数据库、文档数据库、图数据库
* 关系数据库和文档数据库：
  * 前者可以处理多对一和多对多，后者则不太方便
  * 后者处理一对多效率高，前者则需要联接查询
* 文档数据库和图数据库
  * 前者预期数据都来着同一文档，文档与其他文档的关联很少
  * 后者则相反，预期所有的数据都可能互相关联
  * 两者的共同点：不会对存储的数据强加一个模式，使得应用程序易于应对变化的需求
  
#### 第3章 数据存储与检索
* [DDIA逐章精度小册学习【第三章】](https://juejin.cn/post/7191125304285331516)
* CSV：123456,{"name":"a"}\n42,{"name":"b"}
* Bitcask：在CSV的基础上，用内存存储hash索引：{123456:0, 42:20}；数据分段存储，老数据段压缩
* Bitcask的缺点：
  * hash索引很大，内存放不下，放硬盘性能又跟不上
  * 不支持范围查询
* SSTable：每个段的key在该段是唯一且已排序
  * 写入时先写入内存中的红黑树，达到阈值就写入磁盘
  * 内存索引记录每个段的开始key和结束key。查找某个Key时，去所有包含该Key的区间对应的文件二分查找即可。
  * 记录log用于机器崩溃后恢复内存表
* LSM-Tree：基于SSTable的优化
  * 优化SSTable的key的查找：使用布隆过滤器
  * 层级化组织SSTable：策略有大小分级和分层压缩
* B-tree：与LSM-Tree一样，它也支持高效的点查和范围查。
* TP(Transaction Processing)：事务处理
* AP(Analytical Processing)：分析处理
  * 数据仓库：与TP单独分开的数据库
  * 数据模型有星状模型、雪花模型
* 列存储：分析的SQL有时扫描数十亿行，但只关注几个列。如果按行存储则需要读取所有无关的列
* 列压缩：
  * product_sk列的原始数据是：69,69,31,29
  * 位图编码：需要三个位图
    * 69位图存储：1,1,0,0
    * 31位图存储：0,0,1,0
    * 29位图存储：0,0,0,1
  * 游程编码：需要三个游程
    * 69游程存储：0,2 // 两个1，后面都是0
    * 31游程存储：2,1 // 两个0，一个1
    * 29游程存储：3,1 // 三个0，一个1
* 列存储的写入：需要更新当前列，其他列为了保持下标对应，也需要更新

#### 第4章 数据编码与演化
* 向后兼容：当前PB协议可以读取老版本的pb二进制
* 向前兼容：当前PB协议可以读取新版本的pb二进制
* XML：字段类型只能是字符串
* JSON：字段类型只分字符串和数值，但是没有进一步区分数值类型
* Thrift和ProtoBuf都是二进制编码
  * Thrift有数组字段；ProtoBuf的数组是repeated，好处是optional字段可以改成repeated字段
  * 向后兼容：新加的字段必须是optional
  * 向前兼容：只能新加字段，不能删除和修改之前的字段(可修改相容的数据类型)
* Thrift和ProtoBuf修改相容的数据类型：int32改成int64
* Avro没有使用字段标号，Client-Server在通信的握手阶段会先交换数据模式
  * 相比Thrift和ProtoBuf，二进制数据不存储字段标号和字段类型，体积稍小
  * 向后兼容：新代码读取旧数据，首先得到旧数据的写入模式，与当前模式映射，再解读数据
  * 向前兼容：原理同上
  * 映射规则：使用字段名来进行匹配；忽略多出的字段；对缺失字段填默认值
  * 模式演化：只能添加或删除具有默认值的字段；只能修改Avro支持转换的类型

![img](../images/ddia/ch04-fig06.png)

* Avro如何从编码中获取写入模式：
  * 如果一个大文件所有记录都使用相同模式编码，则在文件头包含一次写入模式即可。
  * 数据库在编码时额外记录一个模式版本号，通过版本去查询对应的写入模式即可。
  * 网络通讯时在一个session开始时交换模式，然后在整个session生命周期内都用此模式。
* Avro可以动态解析数据；Thrift和ProtoBuf需要先基于IDL生成代码(好处是可以做代码静态检查)
* 经由数据库的数据流：
  * 问题：新加了一个字段X，新版本的进程把字段X设置为8，旧版本进程不能识别字段X而把值覆盖为空
  * alter table时不允许增加既没有默认值、也不允许为空的列。
* 经由服务的数据流：RESTful和RPC
  * RESTful使用JSON，比较容易添加新的字段来进行演进和兼容；RPC根据编码格式的兼容性规则进行演变；
  * RESTful常将将版本号做到HTTP请求头中；RPC一般提供客户端SDK，升级比较麻烦
* 经由消息传递的数据流
  * 如果消费者暂时不可用，可以充当暂存系统。
  * 当消费者宕机重启后，自动地重新发送消息。
  * 生产者不必知道消费者 IP 和端口。
  * 能将一条消息发送给多个消费者。
  * 将生产者和消费者解耦。

#### 第5章 数据复制
* 数据冗余的好处：
  * 降低延迟：可以在地理上同时接近不同地区的用户。
  * 提高可用性：当系统部分故障时仍然能够正常提供服务。
  * 提高读吞吐：可以水平扩展。
* 常用的冗余控制算法有：
  * 单领导者（single leader）
  * 多领导者（multi-leader）
  * 无领导者（leaderless）
* 数据库冗余问题在学术界不是一个新问题了，但在工业界，大部分人都是新手

##### 单领导者（single leader）
* 新增副本
  * 主副本在本地做一致性快照。
  * 将快照复制到从副本节点。
  * 从副本应用快照，并请求快照点之后的变更日志。
  * 当从副本赶上主副本进度后，就可以正常跟随主副本了。
* 从副本宕机：追赶恢复
  * 落后的多就拉取快照+日志；落后的少就拉取缺失日志。
* 主副本宕机：故障转移的步骤
  * 确认主副本故障。
  * 选择新的主副本。选择数据尽可能新的从副本
  * 让系统感知新主副本。
* 主副本宕机：会遇到的问题
  * 新老主副本数据冲突。新主副本没有同步完所有的日志，老主副本重新上线
  * 新老主副本角色冲突。老主副本重新上线后认为它才是主副本，既发生脑裂
  * 超时阈值选取。过小可能会发生主从频繁切换；过大则使得服务器长时间不可用
* 日志复制：基于语句的复制
  * 非确定性函数问题：NOW()、RAND()
  * 使用自增列，或依赖于现有数据。不同的执行顺序导致副本不一致。
  * 有副作用（触发器、存储过程、自定义函数）的语句
* 日志复制：传输预写日志（WAL）
  * 因为WAL日志和存储引擎绑定，需要注意版本升级的兼容性问题
* 日志复制：逻辑日志复制，与存储引擎无关（例如MySQL的binlog）
  * 对于插入行：日志需包含所有列值。
  * 对于删除行：日志需要包含待删除行标识，如主键
  * 对于更新行：要更新的列值
* 日志复制：基于触发器的复制
  * 由应用层来决策，性能较差且更易出错；但是给了用户更多的灵活性。

##### 复制滞后问题
* 读你所写：刚insert到主副本就去读从副本，但是从副本还没有数据
  * 按内容分类。读自己的资料时，从主副本读取；但读其他人资料时，可以向从副本读。
  * 按时间分类。近期内有过改动的数据，从主副本读，其他的，向从副本读。
  * 利用时间戳。查询时带上上次insert返回的时间戳，从同步时间大于此时间戳的副本读
* 单调读：读取副本A时能查询到记录，读取副本B时查不到
  * 只从一个副本读数据。
  * 利用时间戳。同上
* 一致前缀读：分区1先后记录A和B，分区2先后记录B和A
  * 不分区。
  * 让所有有因果关系的事件路由到一个分区。
* 副本滞后的终极解决方案：分布式事务

##### 多领导者（multi-leader）
* 单个数据中心，多主模型意义不大。适合多主模型的有：
  * 数据库横跨多个数据中心。可以就近写入
  * 需要离线工作的客户端
  * 协同编辑
* 处理写入冲突：冲突避免：
  * 例如根据用户的地理位置来分配主副本
  * 挑战1：用户从一个地点迁移到另一个地点
  * 挑战2：数据中心损坏，导致路由变化
* 处理写入冲突：冲突收敛
  * 给每个写入一个序号，并且后者胜。序号由外部系统生成
  * 例如在Wiki的冲突中，合并后的标题为“B/C”
  * 出现冲突时调用用户自定义的解决冲突策略的代码
* 处理写入冲突：界定冲突
  * 有些冲突显而易见：并发写同一个Key。
  * 有些冲突则更隐晦：例如预定一个会议室，用户A和用户B预定的时间有重叠。
* 三种复制拓扑：
  * 环形拓扑。如果一个节点故障，则可能中断复制链路。
  * 星型拓扑。中心节点负责接受并转发数据。如果中心节点故障，则会使得整个拓扑瘫痪。
  * 全连接拓扑。每个主库都要把数据发给剩余主库。通信链路冗余度较高，能较好的容错。
    * 需要用版本向量的策略，对多个副本的事件进行排序，解决因果一致性问题。

##### 无领导者（leaderless）
* 读时修复和反熵过程
  * 读时修复：在读取时发现旧的就顺手修了
  * 反熵过程：后台进程持续进行扫描，寻找陈旧数据然后更新
* Quorum读写：w+r>n可以保证读请求至少读到一个最新副本，n表示n个全量的副本
* quorum一致性的局限
  * 对于写写并发，如果处理冲突不当时。比如使用last-win策略，根据本地时间戳挑选时，可能由于时钟偏差造成数据丢失。
  * 对于读写并发，写操作仅在部分节点成功就被读取，此时不能确定应当返回新值还是旧值。
  * 如果写入节点数<w导致写入失败，但并没有对数据进行回滚时，客户端读取时，仍然会读到旧的数据。
  * 虽然写入时，成功节点数=w，但中间有故障造成了一些副本宕机，导致成功副本数<w，则在读取时可能会出现问题。

```
# 放松的Quorum和提示转交
Dynamo实际上不需要系统保障所有的quorum节点可用。它使用一种“sloppy quorum”的策略。
简单来说，一份数据储存在ABC三个节点中。当A节点宕机或不可达时，就临时储存在D节点中。
D节点单独有一块区域存储这些本不属于自己的数据，并进行定时轮询。如果发现A节点可用，就将数据传输回去并删掉本地的副本。始终保持三个节点的数据副本。
（注意此处选择D不是任意的，应该是在环中最近一个健康节点，这样才能保证故障时读写数据能找到该节点）。
```







