### TiDB系统管理基础

#### 基础知识
1. [TiDB 系统管理基础 - 视频](https://learn.pingcap.com/learner/course/30002)
1. [18年的压测](https://www.quora.com/How-does-TiDB-compare-with-MySQL)和[19年的压测](https://www.percona.com/blog/2019/01/24/a-quick-look-into-tidb-performance-on-a-single-server/)表明：TiDB的性能只有MySQL的一半
1. [新一代数据库TiDB在美团的实践](https://tech.meituan.com/2018/11/22/mysql-pingcap-practice.html)
  * 美团对TiDB的定位：支持二级索引；跨region failover；跨region双写
  * TiDB对比ClickHouse：ClickHouse跑低频SQL可以，跑高频SQL不行，且跑全量低频SQL会发生overkill；TiDB则可以胜任。
  * 传统分库分表方案的弊端：业务无法友好的执行分布式事务；跨库的查询需要在中间层上组合(不支持跨库join)；再次拆分的成本高
1. [CatKang - NewSQL数据库概述](http://catkang.github.io/2020/12/01/newsql.html)
  * 分库分表：part1是中间件，part2是单机。代表有阿里云DRDS。
  * Spanner：part1是server层，part2是engine层。代表有TiDB。分布式事务的四个特性：
    * Atomicity：单机靠redo+undo；分布式靠redo+undo+2PC
    * Consistency
    * Isolation：单机靠2PL+MVCC(本地时钟)；分布式靠2PL+MVCC(Lamport or TrueTime时钟)
    * Durability：单机靠redo；分布式靠redo+Multi-Paxos
  * Partition Storage：part1是server、engine层，part2是存储。代表有Aurora、PolarDB。

#### redo和undo
* [CatKang - 数据库故障恢复机制的前世今生](http://catkang.github.io/2019/01/16/crash-recovery.html)
* 数据库的四种故障类型：
  * Transaction Failure可能是主动回滚或者冲突后强制Abort；
  * Process Failure指的是由于各种原因导致的进程退出，进程内存内容会丢失；
  * System Failure来源于操作系统或硬件故障；
  * Media Failure则是存储介质的不可恢复损坏。
* WAL和机械硬盘：按Block寻址，os层面则是读一页就算一次IO；随机IO很差
* WBL和固态硬盘：按字节寻址而不是Block；随机IO和顺序IO差不多
* 什么是事务的持久化：事务一旦提交，并对客户端返回success，则数据永久保存。
* [理解数据库中的undo日志、redo日志、检查点](https://blog.csdn.net/Maxiao1204/article/details/107505537)，文章的5、6、7介绍了undo和redo
* [为什么只用 redo-log 或者只用 undo-log 不可以](https://www.jianshu.com/p/57c510f4ec28)
  * 假设只有undo-log：事务提交前数据必须落盘（随机IO），性能差
  * 假设只用redo-log：事务提交前只需要redo落盘（顺序IO）。但是数据落盘的逻辑会很复杂，如果大量数据未落盘，则需要内存；如果要保证实时落盘，则需要保证要落盘的数据页都是commit状态
  * 使用undo+redo：主体流程还是undo-log，但是数据落盘改成redo-log的形式，这样只需要undo、redo落盘即可（顺序IO）。唯一的缺点是写放大，一次事务三次IO。

```
// undo 的流程
日志1记录t=0，数据t=1，标记*
日志2记录t=1，数据t=2
日志3记录t=2，数据t=3，标记*
日志4记录t=3，数据t=4
标记为*后表示事务已提交，此时日志1和日志3可以删除
数据恢复时，只需要恢复日志4和日志2即可

// redo的流程
日志1记录t=1，标记为*，数据t=1
日志2记录t=2
日志3记录t=3，标记为*
日志4记录t=4
标记为*后表示事务已提交，此时需要等数据落盘后才能删除，日志1可以删除，日志3不能
数据恢复时，只需要恢复标记为*日志即可
```

#### 热点问题处理思路
1. [TiDB 最佳实践系列（一）高并发写入常见热点问题及规避方法](https://pingcap.com/blog-cn/tidb-in-high-concurrency-scenarios/)
  * 高并发批量插入会有region热点问题，可以通过预先split region解决
1. [7.2 热点问题处理思路 · TiDB in Action](https://book.tidb.io/session4/chapter7/hotspot-resolved.html)
1. 确认热点问题：Grafana 的 TiKV-Trouble-Shooting 的 Dashboard 的 Hot Read 和 Hot Write 面板
1. 确认热点表或索引：pd-ctl region topwrite|topread 3；TiDB Dashboard的流量可视化。
1. 写入热点的业务场景通常有：
  * 有自增主键：去掉自增主键并设置 SHARD_ROW_ID_BITS（可动态设置，SHARD_ROW_ID_BITS=4表示16个分片）。
  * 存在递增索引，比如时间索引等：手工切分热点 Region。
  * 高并发更新小表：改造小表为 hash 分区表。
  * 秒杀场景下的单行热点问题：业务层面用内存缓存解决。

#### TiKV 架构
1. [TiKV 简介 | PingCAP Docs](https://docs.pingcap.com/zh/tidb/stable/tikv-overview)
1. [RocksDB 简介 | PingCAP Docs](https://docs.pingcap.com/zh/tidb/stable/rocksdb-overview)
1. Region 副本(Peer)的三个角色：Leader负责读写可投票；Follower可随时替换Leader可投票；Learner是不完整的副本，不可投票。
1. 每个 TiKV 实例中有两个 RocksDB 实例
  * 一个用于存储 Raft 日志（通常被称为 raftdb）
  * 一个用于存储用户数据以及 MVCC 信息（通常被称为 kvdb）

#### 2.讲存储
1. [TiDB 高并发写入场景最佳实践](https://docs.pingcap.com/zh/tidb/stable/high-concurrency-best-practices)
1. [TiDB Best Practice](https://pingcap.com/blog-cn/tidb-best-practice/)
1. [三篇文章了解 TiDB 技术内幕 - 讲存储](https://pingcap.com/blog-cn/tidb-internal-1)
1. Raft 是一个一致性协议，提供几个重要的功能：
  * Leader 选举
  * 成员变更
  * 日志复制
1. 每个数据变更都会落地为一条 Raft 日志，再通过 Raft 的日志来同步副本数据

#### 3.说计算
* [三篇文章了解 TiDB 技术内幕 - 说计算](https://pingcap.com/blog-cn/tidb-internal-2)
* TiDB 最底层用 Raft 来同步数据。每次写入都要写入多数副本，才能对外返回成功。
* 比如最大 3 副本的话，每次写入 2 副本才算成功。写入的延迟取决于最快的两个副本，而不是最慢的那个副本。
* 分布式事务采用的乐观锁。缺点只有在真正提交的时候，才会做冲突检测，所以在冲突严重的场景下性能低下。
* 数据表的数据或者索引具有相同的前缀，这些 Key-Value 会在相邻的位置。批量写入会在很少的几个 Region 上形成写入热点，成为整个系统的瓶颈。
* 二级索引的特点：尽量用区分度比较大的列；最左原则；数据分散在很多Region上，并发查询的Region数可配
* 建议每个事务的行数不超过 200 行，且单行数据小于 100k，否则可能性能不佳。

```
主键索引：key：tableID、rowID；value：\[col1, col2, col3, col4...]
唯一索引：key：tableID、indexID、indexedColumnsValue；value：rowID
非唯一索引：key：tableID、indexID、indexedColumnsValue、rowID；value：null

// 举例有张Age，tableID=10，它的非唯一索引indexID=1，它的唯一索引indexID=2，有三条数据如下
ID Name    Role         Age
1, "TiDB", "SQL Layer", 10
2, "TiKV", "KV Engine", 20
3, "PD",   "Manager",   30

// 主键索引
t10_r1 --> ["TiDB", "SQL Layer", 10]
t10_r2 --> ["TiKV", "KV Engine", 20]
t10_r3 --> ["PD", "Manager", 30]

// 唯一索引[Name]
t10_i2_TiDB --> 1
t10_i2_TiKV --> 2
t10_i2_PB --> 3

// 非唯一索引[Age]
t10_i1_10_1 --> null
t10_i1_20_2 --> null
t10_i1_30_3 --> null
```

#### 4.谈调度
1. [三篇文章了解 TiDB 技术内幕 - 谈调度](https://pingcap.com/blog-cn/tidb-internal-3/)
1. [TiDB 最佳实践系列（二）PD 调度策略最佳实践](https://pingcap.com/blog-cn/best-practice-pd)
1. 相关概念
  * Store：即TiKV实例
  * Region：若Region有3个副本，也即3个Peer。每个Region有1个raft实例，也称Raft Group。
  * Pending / Down：是Peer可能出现的两种特殊状态
    * Pending 表示 Follower 或 Learner 的 raft log 与 Leader 有较大差距，Pending 状态的 Follower 无法被选举成 Leader。
    * Down 是指 Leader 长时间没有收到对应 Peer 的消息，可能是宕机或者网络隔离。
  * Scheduler：是PD的调度器，常用的调度器有：
    * balance-leader-scheduler：保持不同节点的 Leader 均衡。
    * balance-region-scheduler：保持不同节点的 Peer 均衡。
    * hot-region-scheduler：保持不同节点的读写热点 Region 均衡。
    * evict-leader-{store-id}：驱逐某个节点的所有 Leader。（常用于滚动升级）
1. TiKV 节点周期性地向 PD 上报 StoreHeartbeat 和 RegionHeartbeat 两种心跳消息。
1. StoreHeartbeat 包含了 Store 的基本信息，由 Store 定期向 PD 汇报。
  * 总磁盘容量
  * 可用磁盘容量
  * 承载的 Region 数量
  * 数据读写速度
  * 发送/接受的 Snapshot 数量（副本之间可能会通过 Snapshot 同步数据）
  * 是否过载
  * 标签信息
1. RegionHeartbeat 包含了 Region 的相关信息，由 Raft Group 的 Leader 定期向 PD 汇报。
  * Leader 的位置
  * Followers 的位置
  * 掉线副本的个数
  * 数据读写速度

#### 5.TiDB 和 MySQL 的区别
1. MySQL 的时区由环境变量 TZ 或命令行参数 --timezone 决定
1. TiDB 的时区由 TiDB 节点环境变量配置 TZ 决定

#### 6.TiDB 事务模型
1. [6.TiDB 事务模型 · TiDB in Action](https://book.tidb.io/session1/chapter6/tidb-transaction-mode.html)
1. 快照隔离。TiDB 使用 PD 作为全局授时服务（TSO）来提供单调递增的版本号（timestamp）
  * 事务开始时获取 start timestamp
  * 事务提交时获取 commit timestamp，同时也是数据的版本号
  * 事务只能读到在事务 start timestamp 之前的数据（已提交的）
  * 事务在提交时会根据 timestamp 来检测数据冲突
1. TiDB 开始两阶段提交
  * TiDB 向 TiKV 发起 Prewrite 请求。TiKV 检查冲突并加锁。
  * TiDB 收到所有 Prewrite 响应且所有 Prewrite 都成功。
  * TiDB 向 TiKV 发起第二阶段提交。TiKV 执行提交。
  * TiDB 收到所有成功响应则 Success，否则回滚。
1. 乐观锁大事务的缺点：一个事务内包含向10000人转账，过程中有另一个事务向其中一人转账并成功，此大事务提交失败并回滚。
1. 悲观锁性能不如乐观锁：事务内每个 DML 时都需要向 TiKV 发送加锁请求（建议将多条 DML 合并成 一条）
1. 4.0 版本之前对大事务有严格限制，原因有：
  * Prewrite 写下的锁会阻塞其他事务的读，Prewrite 时间长，阻塞的时间也就长。
  * 大事务 Prewrite 时间长，可能会被其他事务终止导致提交失败。

#### 7.TiDB DDL
1. [7.TiDB DDL · TiDB in Action](https://book.tidb.io/session1/chapter7/tidb-ddl-intro.html)
1. 表结构设计最佳实践
  * 设置 SHARD_ROW_ID_BITS 来把 rowID 打散写入多个不同的 Region 中
  * 设置 AUTO_RANDOM 代替 AUTO_INCREMENT 插入数据时自动为整型分配一个随机值
  * 4.0 版本的 PD 会提供 Load Based Splitting 策略，除了根据 Region 的大小进行分裂，还可以根据 QPS。
  * 按日期删除老数据，正常的删除会很慢。如果按日期建立分区表则很快：避免了往TiKV写delete记录，避免了RocksDB的compaction引发的抖动
  * 修改表不允许降低字段长度
  * 不要设计过大的宽表
1. DDL命令：
  * ADMIN SHOW DDL JOBS 5; 显示最近5条DDL命令
  * ADMIN SHOW DDL JOB QUERIES {job_id}; 显示DDL详细信息
  * ADMIN CANCEL DDL JOBS {job_id}; 取消DDL命令
1. DDL 处理流程：
  * 每个 DDL 命令有一个 job_id 并保存在 tikv
  * TiDB 实例会竞选出一个 Owner 节点来执行实际 DDL 任务
  * 两个队列：ADD_INDEX 和非 ADD_INDEX；job_id 小的 DLL 先执行；
1. DDL 变更原理
  * schema 最多有两个版本，版本状态有absent、delete only、write only、public
  * 删除操作如DROP INDEX，DROP TABLE，DROP DATABASE，TRUNCATE TABLE等，先记录到gc_delete_range表，再通过GC机制删除
  * DELETE COLUMN代价比较大，所以只在schema上标记删除
1. [DDL 变更原理](https://book.tidb.io/session1/chapter7/tidb-ddl-status.html)
  * Add column operation：只更新schema，新的row是新结构，查询老的row就依据schema的默认值返回
  * Modify column operation：转换column的类型只支持整型(lengthened)；auto_increment只能在新建表的时候设置；如果索引有用到此column，索引的schema也需改变(但原始数据不变)
  * Add index operation：先生成好index再把index设置为可用，耗时较长
1. Sequence 自增序列
  * CREATE SEQUENCE seq_for_unique START WITH 1 INCREMENT BY 1 CACHE 1000 NOCYCLE; 创建序列
  * SHOW CREATE SEQUENCE seq_for_unique；获取创建序列的SQL
  * DROP SEQUENCE seq_for_unique；删除序列
  * SELECT NEXT/PREVIOUS VALUE FOR seq_for_unique; 获取下一个/上一个自增值
  * CREATE TABLE user ( auto_id int(11) DEFAULT 'nextval(test.seq_for_unique)' ); 表字段使用自增序列
1. AutoRandom 只能用在主键上，不能用在唯一索引

#### TiDB 集群管理
* 安装：pd是大脑（3台）、tidb是无状态的server（2台）、tikv（3台，默认3个副本）
* 安装：需先安装numactl。tiup cluster template > topology.yaml，去掉TiFlash配置
* 配置分为系统配置（存在tikv中，有作用域，session只对当前会话生效，global对新启的会话生效）、集群配置（tidb、tikv、pd，需改配置文件并重启）
* mysql分用户和角色，attach角色后，用户登录需要执行set role all




